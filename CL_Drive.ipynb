{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/papertuc2000/CL-Drive/blob/dev/CL_Drive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CL-Drive Multi-Modal Data Generator\n",
        "# ------------------------------------------------------------\n",
        "# This generator:\n",
        "#   • Automatically scans dataset structure\n",
        "#   • Synchronizes modalities per (participant, level)\n",
        "#   • Ensures label alignment\n",
        "#   • Performs sliding-window segmentation\n",
        "#   • Resamples all signals to a unified sampling rate\n",
        "#   • Supports classification or regression\n",
        "#   • Uses file-level caching for speed optimization\n",
        "#   • Returns dictionary input compatible with fusion models\n",
        "# ==="
      ],
      "metadata": {
        "id": "uWZAwE48D1n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Connect to Google Drive ---\n",
        "print(\"Connecting to Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "cQm1p5kc5yxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985532a4-2f60-4f88-8018-05ff85444c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from glob import glob\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from scipy.signal import resample"
      ],
      "metadata": {
        "id": "lmMJlqtiDi3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from glob import glob\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from scipy.signal import resample\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# CL-Drive Multi-Modal Data Generator (FINAL REVISED)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "class CLDriveMultiModalGenerator(Sequence):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_path,\n",
        "        modalities=('ECG', 'EEG', 'EDA', 'Gaze'),\n",
        "        batch_size=8,\n",
        "        window_sec=10,\n",
        "        target_fs=128,\n",
        "        shuffle=True,\n",
        "        task='classification',\n",
        "        n_classes=3,\n",
        "        use_cache=True\n",
        "    ):\n",
        "\n",
        "        self.dataset_path = dataset_path\n",
        "        self.modalities = modalities\n",
        "        self.batch_size = batch_size\n",
        "        self.window_sec = window_sec\n",
        "        self.target_fs = target_fs\n",
        "        self.shuffle = shuffle\n",
        "        self.task = task\n",
        "        self.n_classes = n_classes\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "        # Original sampling rates\n",
        "        self.fs_dict = {\n",
        "            'ECG': 512,\n",
        "            'EEG': 256,\n",
        "            'EDA': 128,\n",
        "            'Gaze': 50\n",
        "        }\n",
        "\n",
        "        self.cache = {}\n",
        "\n",
        "        # Build aligned dataset\n",
        "        self.samples = self._build_index()\n",
        "\n",
        "        # Build window index\n",
        "        self.indices = self._create_windows()\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    # =====================================================\n",
        "    # 1️⃣  Correct Alignment Logic (Your Exact Structure)\n",
        "    # =====================================================\n",
        "    def _build_index(self):\n",
        "\n",
        "        aligned_data = {}\n",
        "        label_files = {}\n",
        "\n",
        "        all_files = glob(\n",
        "            os.path.join(self.dataset_path, \"**\", \"*.csv\"),\n",
        "            recursive=True\n",
        "        )\n",
        "\n",
        "        for f_path in all_files:\n",
        "\n",
        "            f_path = f_path.replace(\"\\\\\", \"/\")\n",
        "\n",
        "            # Skip baseline\n",
        "            if \"baseline\" in f_path.lower():\n",
        "                continue\n",
        "\n",
        "            parts = f_path.split(\"/\")\n",
        "            filename = parts[-1]\n",
        "\n",
        "            # -------------------------\n",
        "            # LABEL FILE\n",
        "            # -------------------------\n",
        "            if \"/Labels/\" in f_path:\n",
        "                participant = os.path.basename(f_path).split('.')[0]\n",
        "                label_files[participant] = f_path\n",
        "                continue\n",
        "\n",
        "            # -------------------------\n",
        "            # MODALITY FILE\n",
        "            # -------------------------\n",
        "            modality = parts[-3]        # e.g. EEG\n",
        "            participant = parts[-2]     # e.g. participant_ID_1\n",
        "\n",
        "            if modality not in self.modalities:\n",
        "                continue\n",
        "\n",
        "            # Extract level\n",
        "            if \"level_\" not in filename:\n",
        "                continue\n",
        "\n",
        "            level = filename.split(\"level_\")[-1].replace(\".csv\", \"\")\n",
        "\n",
        "            group_key = (participant, level)\n",
        "\n",
        "            if group_key not in aligned_data:\n",
        "                aligned_data[group_key] = {}\n",
        "\n",
        "            aligned_data[group_key][modality] = f_path\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # 2️⃣  FILTER: Keep ONLY complete modality sets\n",
        "        # --------------------------------------------------\n",
        "        final_samples = []\n",
        "\n",
        "        for (participant, level), found_modalities in aligned_data.items():\n",
        "\n",
        "            missing = set(self.modalities) - set(found_modalities.keys())\n",
        "\n",
        "            # Skip if any modality missing\n",
        "            if len(missing) > 0:\n",
        "                print(f\"Skipping {participant} Level {level}: Missing {missing}\")\n",
        "                continue\n",
        "\n",
        "            # Skip if no label\n",
        "            if participant not in label_files:\n",
        "                print(f\"Skipping {participant} Level {level}: Missing label file\")\n",
        "                continue\n",
        "\n",
        "            final_samples.append({\n",
        "                'participant': participant,\n",
        "                'level': int(level),\n",
        "                'paths': found_modalities,\n",
        "                'label_path': label_files[participant]\n",
        "            })\n",
        "\n",
        "        print(f\"\\n✔ Total fully synchronized samples: {len(final_samples)}\\n\")\n",
        "\n",
        "        return final_samples\n",
        "\n",
        "    # =====================================================\n",
        "    # 2️⃣  Window Construction\n",
        "    # =====================================================\n",
        "    def _create_windows(self):\n",
        "\n",
        "        window_indices = []\n",
        "\n",
        "        for sample_idx, sample in enumerate(self.samples):\n",
        "\n",
        "            label_df = self._load_csv(sample['label_path'])\n",
        "            n_segments = len(label_df)\n",
        "\n",
        "            for seg_idx in range(n_segments):\n",
        "                window_indices.append((sample_idx, seg_idx))\n",
        "\n",
        "        return window_indices\n",
        "\n",
        "    # =====================================================\n",
        "    # 3️⃣  CSV Loader (File-level Cache)\n",
        "    # =====================================================\n",
        "    def _load_csv(self, path):\n",
        "\n",
        "        if self.use_cache:\n",
        "            if path not in self.cache:\n",
        "                self.cache[path] = pd.read_csv(path)\n",
        "            return self.cache[path]\n",
        "        else:\n",
        "            return pd.read_csv(path)\n",
        "\n",
        "    # =====================================================\n",
        "    # 4️⃣  Resampling\n",
        "    # =====================================================\n",
        "    def _resample_signal(self, signal, orig_fs):\n",
        "\n",
        "        target_length = int(self.window_sec * self.target_fs)\n",
        "\n",
        "        if len(signal) == 0:\n",
        "            return np.zeros((target_length, signal.shape[1]))\n",
        "\n",
        "        return resample(signal, target_length)\n",
        "\n",
        "    # =====================================================\n",
        "    # 5️⃣  Segment Extraction\n",
        "    # =====================================================\n",
        "    def _load_segment(self, path, modality, seg_idx):\n",
        "\n",
        "        df = self._load_csv(path)\n",
        "        signal = df.values\n",
        "        orig_fs = self.fs_dict[modality]\n",
        "\n",
        "        start = int(seg_idx * self.window_sec * orig_fs)\n",
        "        end = int((seg_idx + 1) * self.window_sec * orig_fs)\n",
        "\n",
        "        segment = signal[start:end]\n",
        "\n",
        "        if segment.shape[0] == 0:\n",
        "            segment = np.zeros((1, signal.shape[1]))\n",
        "\n",
        "        segment = self._resample_signal(segment, orig_fs)\n",
        "\n",
        "        return segment.astype(np.float32)\n",
        "\n",
        "    # =====================================================\n",
        "    # 6️⃣  Label Processing\n",
        "    # =====================================================\n",
        "    def _process_label(self, label_value):\n",
        "\n",
        "        if self.task == 'classification':\n",
        "            cls = 0\n",
        "            if label_value < 4:\n",
        "                cls = 0\n",
        "            elif label_value < 7:\n",
        "                cls = 1\n",
        "            elif label_value < 10:\n",
        "                cls = 2\n",
        "\n",
        "            return tf.keras.utils.to_categorical(\n",
        "                cls,\n",
        "                self.n_classes\n",
        "            )\n",
        "        else:\n",
        "            return np.array([label_value], dtype=np.float32)\n",
        "\n",
        "    # =====================================================\n",
        "    # 7️⃣  Required by Keras\n",
        "    # =====================================================\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    # =====================================================\n",
        "    # 8️⃣  Batch Generation\n",
        "    # =====================================================\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        batch_indices = self.indices[\n",
        "            index * self.batch_size:(index + 1) * self.batch_size\n",
        "        ]\n",
        "\n",
        "        X_batches = {f\"{m.lower()}_input\": [] for m in self.modalities}\n",
        "        y_batch = []\n",
        "\n",
        "        for sample_idx, seg_idx in batch_indices:\n",
        "\n",
        "            sample = self.samples[sample_idx]\n",
        "\n",
        "            # Load modalities\n",
        "            for m in self.modalities:\n",
        "                segment = self._load_segment(\n",
        "                    sample['paths'][m],\n",
        "                    m,\n",
        "                    seg_idx\n",
        "                )\n",
        "                X_batches[f\"{m.lower()}_input\"].append(segment)\n",
        "\n",
        "            # Load label\n",
        "            label_df = self._load_csv(sample['label_path'])\n",
        "            label_lvl = (os.path.basename(sample['paths']['ECG']).split('.')[0]).split('_')[-1]\n",
        "            label_value = label_df[f'lvl_{label_lvl}'].iloc[seg_idx]\n",
        "            y_batch.append(self._process_label(label_value))\n",
        "\n",
        "        X = {k: np.array(v, dtype=np.float32) for k, v in X_batches.items()}\n",
        "        y = np.array(y_batch)\n",
        "\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "NqcUq5Io56bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path='/content/drive/MyDrive/Colab Notebooks/CL-Drive'"
      ],
      "metadata": {
        "id": "VLPghVgSpFwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modalities = ['ECG', 'EEG', 'EDA', 'Gaze']"
      ],
      "metadata": {
        "id": "d3v5N9cDpI-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modalities_path_dic = {f'{m}': [] for m in modalities}\n",
        "modalities_path_dic.update({'Labels': []})"
      ],
      "metadata": {
        "id": "8hpwge_OpPY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_files = glob(os.path.join(dataset_path, \"**\", \"*.csv\"), recursive=True)"
      ],
      "metadata": {
        "id": "JGbvL7mJpRX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = all_files[0].split(os.sep)\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o30b0Az7pS_R",
        "outputId": "65e5cd36-23cc-4838-f5a0-7f592d9b6b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'content',\n",
              " 'drive',\n",
              " 'MyDrive',\n",
              " 'Colab Notebooks',\n",
              " 'CL-Drive',\n",
              " 'ECG',\n",
              " '1372',\n",
              " 'ecg_data_level_1.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for f_path in all_files:\n",
        "    temp = f_path.split(os.sep)\n",
        "    modalities_path_dic[temp[6]].append(f_path)"
      ],
      "metadata": {
        "id": "to31q0iXpDLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "modalities = ['ECG', 'EEG', 'EDA', 'Gaze', 'Labels']\n",
        "dataset_path = \"D:/Projects/PythonProjects/BioSignalClassification/CL-Drive\" # Example path\n",
        "\n",
        "# 1. Nested storage: { (participant, level): { 'EEG': path, 'ECG': path ... } }\n",
        "aligned_data = {}\n",
        "\n",
        "all_files = glob(os.path.join(dataset_path, \"**\", \"*.csv\"), recursive=True)\n",
        "\n",
        "for f_path in all_files:\n",
        "    f_path = f_path.replace(\"\\\\\", \"/\")\n",
        "    parts = f_path.split(\"/\")\n",
        "\n",
        "    # Extract info from path (Adjust indices based on your exact root)\n",
        "    # Example: .../EEG/participant_ID_1/eeg_data_level_9.csv\n",
        "    modality = parts[-3]       # e.g., 'EEG'\n",
        "    participant = parts[-2]    # e.g., 'participant_ID_1'\n",
        "    filename = parts[-1]       # e.g., 'eeg_data_level_9.csv'\n",
        "\n",
        "    # Extract level (assuming 'level_X' is always in the filename)\n",
        "    # We use a simple split or regex to get the number\n",
        "    level = filename.split('level_')[-1].replace('.csv', '')\n",
        "\n",
        "    # Initialize the group key\n",
        "    group_key = (participant, level)\n",
        "    if group_key not in aligned_data:\n",
        "        aligned_data[group_key] = {}\n",
        "\n",
        "    aligned_data[group_key][modality] = f_path\n",
        "\n",
        "# 2. Filter: Only keep groups that have ALL modalities\n",
        "final_paths = {m: [] for m in modalities}\n",
        "\n",
        "for (participant, level), found_modalities in aligned_data.items():\n",
        "    # Check if every required modality exists for this specific participant+level\n",
        "    if all(m in found_modalities for m in modalities):\n",
        "        for m in modalities:\n",
        "            final_paths[m].append(found_modalities[m])\n",
        "    else:\n",
        "        print(f\"Skipping {participant} Level {level}: Missing {set(modalities) - set(found_modalities)}\")"
      ],
      "metadata": {
        "id": "g_cFsV-mqJBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = CLDriveMultiModalGenerator(\n",
        "    dataset_path='/content/drive/MyDrive/Colab Notebooks/CL-Drive',\n",
        "    batch_size=16,\n",
        "    window_sec=10,\n",
        "    target_fs=128,\n",
        ")"
      ],
      "metadata": {
        "id": "785KgdAzW2ae",
        "outputId": "0aec485f-1950-4486-94fa-fb79c34af8de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping 1744 Level 7: Missing {'EEG'}\n",
            "Skipping 1716 Level 2: Missing {'Gaze'}\n",
            "Skipping 1716 Level 3: Missing {'Gaze'}\n",
            "Skipping 1716 Level 4: Missing {'Gaze'}\n",
            "Skipping 1716 Level 7: Missing {'Gaze'}\n",
            "Skipping 1716 Level 8: Missing {'Gaze'}\n",
            "Skipping 1547 Level 1: Missing {'Gaze'}\n",
            "Skipping 1547 Level 2: Missing {'Gaze'}\n",
            "Skipping 1547 Level 7: Missing {'Gaze'}\n",
            "Skipping 1547 Level 8: Missing {'Gaze'}\n",
            "Skipping 1323 Level 4: Missing {'EEG'}\n",
            "Skipping 1323 Level 2: Missing {'ECG', 'EDA'}\n",
            "\n",
            "✔ Total fully synchronized samples: 171\n",
            "\n"
          ]
        }
      ]
    }
  ]
}