{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/papertuc2000/CL-Drive/blob/dev/CL_Drive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CL-Drive Multi-Modal Data Generator\n",
        "# ------------------------------------------------------------\n",
        "# This generator:\n",
        "#   • Automatically scans dataset structure\n",
        "#   • Synchronizes modalities per (participant, level)\n",
        "#   • Ensures label alignment\n",
        "#   • Performs sliding-window segmentation\n",
        "#   • Resamples all signals to a unified sampling rate\n",
        "#   • Supports classification or regression\n",
        "#   • Uses file-level caching for speed optimization\n",
        "#   • Returns dictionary input compatible with fusion models\n",
        "# ==="
      ],
      "metadata": {
        "id": "uWZAwE48D1n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Connect to Google Drive ---\n",
        "print(\"Connecting to Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "cQm1p5kc5yxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08f26f8-0651-44fe-a30e-eb1c49df563f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive...\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from glob import glob\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from scipy.signal import resample"
      ],
      "metadata": {
        "id": "lmMJlqtiDi3n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class CLDriveMultiModalGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Keras-compatible multi-modal generator for CL-Drive dataset.\n",
        "\n",
        "    Each batch contains:\n",
        "        X = {\n",
        "            'ecg_input':  (B, T, C1),\n",
        "            'eeg_input':  (B, T, C2),\n",
        "            'eda_input':  (B, T, C3),\n",
        "            'gaze_input': (B, T, C4)\n",
        "        }\n",
        "        y = (B, n_classes) or (B, 1)\n",
        "\n",
        "    Where:\n",
        "        B = batch size\n",
        "        T = window_sec * target_fs (unified temporal length)\n",
        "        Cx = channel dimension of each modality\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_path,\n",
        "        modalities=('ECG', 'EEG', 'EDA', 'Gaze'),\n",
        "        batch_size=8,\n",
        "        window_sec=10,\n",
        "        target_fs=128,\n",
        "        shuffle=True,\n",
        "        task='classification',   # 'classification' or 'regression'\n",
        "        n_classes=3,\n",
        "        require_all_modalities=True,\n",
        "        use_cache=True\n",
        "    ):\n",
        "\n",
        "        # ----------------------------\n",
        "        # Basic configuration\n",
        "        # ----------------------------\n",
        "        self.dataset_path = dataset_path\n",
        "        self.modalities = modalities\n",
        "        self.batch_size = batch_size\n",
        "        self.window_sec = window_sec\n",
        "        self.target_fs = target_fs\n",
        "        self.shuffle = shuffle\n",
        "        self.task = task\n",
        "        self.n_classes = n_classes\n",
        "        self.require_all_modalities = require_all_modalities\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "        # ----------------------------\n",
        "        # Original sampling rates\n",
        "        # These are used to compute window boundaries\n",
        "        # before resampling to target_fs\n",
        "        # ----------------------------\n",
        "        self.fs_dict = {\n",
        "            'ECG': 512,\n",
        "            'EEG': 256,\n",
        "            'EDA': 128,\n",
        "            'Gaze': 50\n",
        "        }\n",
        "\n",
        "        # ----------------------------\n",
        "        # File-level cache:\n",
        "        # Stores loaded CSV files in memory to\n",
        "        # avoid repeated disk reads during training\n",
        "        # ----------------------------\n",
        "        self.cache = {}\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Step 1: Build synchronized dataset index\n",
        "        # --------------------------------------------------\n",
        "        self.samples = self._build_index()\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Step 2: Create window index mapping\n",
        "        # (sample_idx, segment_idx)\n",
        "        # --------------------------------------------------\n",
        "        self.indices = self._create_windows()\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    # =====================================================\n",
        "    # Dataset Synchronization\n",
        "    # =====================================================\n",
        "    def _build_index(self):\n",
        "        \"\"\"\n",
        "        Scans dataset directory recursively and builds\n",
        "        synchronized multi-modal index.\n",
        "\n",
        "        Synchronization logic:\n",
        "            key = (participant_id, level)\n",
        "\n",
        "        Only samples satisfying:\n",
        "            - label file exists\n",
        "            - required modalities exist\n",
        "        are included.\n",
        "        \"\"\"\n",
        "\n",
        "        # Collect all CSV files recursively\n",
        "        all_files = glob(os.path.join(self.dataset_path, \"**\", \"*.csv\"), recursive=True)\n",
        "\n",
        "        synced_data = defaultdict(dict)\n",
        "        label_files = {}\n",
        "\n",
        "        for file_path in all_files:\n",
        "\n",
        "            # Ignore baseline recordings\n",
        "            if 'baseline' in file_path.lower():\n",
        "                continue\n",
        "\n",
        "            parts = file_path.split(os.sep)\n",
        "            filename = parts[-1]\n",
        "\n",
        "            # --------------------------\n",
        "            # Detect label files\n",
        "            # One label file per participant\n",
        "            # --------------------------\n",
        "            if f\"{os.sep}Labels{os.sep}\" in file_path:\n",
        "                p_id = parts[-2]\n",
        "                label_files[p_id] = file_path\n",
        "                continue\n",
        "\n",
        "            # --------------------------\n",
        "            # Detect modality files\n",
        "            # --------------------------\n",
        "            for m in self.modalities:\n",
        "                if f\"{os.sep}{m}{os.sep}\" in file_path:\n",
        "\n",
        "                    p_id = parts[-2]\n",
        "\n",
        "                    # Extract difficulty level from filename\n",
        "                    if '_level_' in filename:\n",
        "                        level = filename.split('_level_')[-1].replace('.csv', '')\n",
        "                        key = (p_id, level)\n",
        "                        synced_data[key][m] = file_path\n",
        "\n",
        "                    break\n",
        "\n",
        "        # --------------------------\n",
        "        # Final filtering\n",
        "        # --------------------------\n",
        "        final_samples = []\n",
        "\n",
        "        for (p_id, level), files in synced_data.items():\n",
        "\n",
        "            # Must have labels\n",
        "            if p_id not in label_files:\n",
        "                continue\n",
        "\n",
        "            # Strict multi-modal requirement\n",
        "            if self.require_all_modalities:\n",
        "                if not all(m in files for m in self.modalities):\n",
        "                    continue\n",
        "            else:\n",
        "                # Minimal requirement: ECG present\n",
        "                if 'ECG' not in files:\n",
        "                    continue\n",
        "\n",
        "            final_samples.append({\n",
        "                'p_id': p_id,\n",
        "                'level': int(level),\n",
        "                'paths': files,\n",
        "                'label_path': label_files[p_id]\n",
        "            })\n",
        "\n",
        "        print(f\"Total synchronized samples: {len(final_samples)}\")\n",
        "\n",
        "        return final_samples\n",
        "\n",
        "    # =====================================================\n",
        "    # Window Construction\n",
        "    # =====================================================\n",
        "    def _create_windows(self):\n",
        "        \"\"\"\n",
        "        Each subjective label corresponds to one\n",
        "        10-second segment.\n",
        "\n",
        "        This method creates index pairs:\n",
        "            (sample_idx, segment_idx)\n",
        "        \"\"\"\n",
        "\n",
        "        window_indices = []\n",
        "\n",
        "        for sample_idx, sample in enumerate(self.samples):\n",
        "\n",
        "            # Load label file to determine number of segments\n",
        "            label_df = self._load_csv(sample['label_path'])\n",
        "            n_segments = len(label_df)\n",
        "\n",
        "            for seg_idx in range(n_segments):\n",
        "                window_indices.append((sample_idx, seg_idx))\n",
        "\n",
        "        return window_indices\n",
        "\n",
        "    # =====================================================\n",
        "    # CSV Loader with Optional Caching\n",
        "    # =====================================================\n",
        "    def _load_csv(self, path):\n",
        "        \"\"\"\n",
        "        Loads CSV file with optional in-memory caching.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.use_cache:\n",
        "            if path not in self.cache:\n",
        "                self.cache[path] = pd.read_csv(path)\n",
        "            return self.cache[path]\n",
        "        else:\n",
        "            return pd.read_csv(path)\n",
        "\n",
        "    # =====================================================\n",
        "    # Required by Keras Sequence\n",
        "    # =====================================================\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns number of batches per epoch.\n",
        "        \"\"\"\n",
        "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Shuffle window indices at epoch end\n",
        "        to prevent ordering bias.\n",
        "        \"\"\"\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    # =====================================================\n",
        "    # Resampling\n",
        "    # =====================================================\n",
        "    def _resample_signal(self, signal, orig_fs):\n",
        "        \"\"\"\n",
        "        Resamples signal to unified target_fs.\n",
        "\n",
        "        Ensures that all modalities share\n",
        "        identical temporal length:\n",
        "            window_sec * target_fs\n",
        "        \"\"\"\n",
        "\n",
        "        target_length = int(self.window_sec * self.target_fs)\n",
        "\n",
        "        if len(signal) == 0:\n",
        "            return np.zeros((target_length, signal.shape[1]))\n",
        "\n",
        "        return resample(signal, target_length)\n",
        "\n",
        "    # =====================================================\n",
        "    # Segment Extraction\n",
        "    # =====================================================\n",
        "    def _load_segment(self, path, modality, seg_idx):\n",
        "        \"\"\"\n",
        "        Extracts a fixed-length time window corresponding\n",
        "        to the seg_idx-th subjective label.\n",
        "        \"\"\"\n",
        "\n",
        "        df = self._load_csv(path)\n",
        "        signal = df.values\n",
        "\n",
        "        orig_fs = self.fs_dict[modality]\n",
        "\n",
        "        # Compute temporal window boundaries\n",
        "        start = int(seg_idx * self.window_sec * orig_fs)\n",
        "        end = int((seg_idx + 1) * self.window_sec * orig_fs)\n",
        "\n",
        "        segment = signal[start:end]\n",
        "\n",
        "        # Resample to target frequency\n",
        "        segment = self._resample_signal(segment, orig_fs)\n",
        "\n",
        "        return segment.astype(np.float32)\n",
        "\n",
        "    # =====================================================\n",
        "    # Label Processing\n",
        "    # =====================================================\n",
        "    def _process_label(self, label_value):\n",
        "        \"\"\"\n",
        "        Converts raw subjective score into:\n",
        "            - one-hot class vector (classification)\n",
        "            - scalar value (regression)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.task == 'classification':\n",
        "            return tf.keras.utils.to_categorical(label_value, self.n_classes)\n",
        "        else:\n",
        "            return np.array([label_value], dtype=np.float32)\n",
        "\n",
        "    # =====================================================\n",
        "    # Batch Construction\n",
        "    # =====================================================\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Generates one batch of multi-modal data.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_indices = self.indices[\n",
        "            index * self.batch_size:(index + 1) * self.batch_size\n",
        "        ]\n",
        "\n",
        "        # Initialize per-modality containers\n",
        "        X_batches = {f\"{m.lower()}_input\": [] for m in self.modalities}\n",
        "        y_batch = []\n",
        "\n",
        "        for sample_idx, seg_idx in batch_indices:\n",
        "\n",
        "            sample = self.samples[sample_idx]\n",
        "\n",
        "            # --------------------------\n",
        "            # Load each modality window\n",
        "            # --------------------------\n",
        "            for m in self.modalities:\n",
        "                segment = self._load_segment(\n",
        "                    sample['paths'][m],\n",
        "                    m,\n",
        "                    seg_idx\n",
        "                )\n",
        "                X_batches[f\"{m.lower()}_input\"].append(segment)\n",
        "\n",
        "            # --------------------------\n",
        "            # Load corresponding label\n",
        "            # --------------------------\n",
        "            label_df = self._load_csv(sample['label_path'])\n",
        "            label_value = label_df.iloc[seg_idx].values[0]\n",
        "            label = self._process_label(label_value)\n",
        "\n",
        "            y_batch.append(label)\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        X = {\n",
        "            k: np.array(v, dtype=np.float32)\n",
        "            for k, v in X_batches.items()\n",
        "        }\n",
        "\n",
        "        y = np.array(y_batch)\n",
        "\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "NqcUq5Io56bK"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}